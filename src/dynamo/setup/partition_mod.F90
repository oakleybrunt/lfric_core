!-------------------------------------------------------------------------------
! (c) The copyright relating to this work is owned jointly by the Crown,
! Met Office and NERC 2014.
! However, it has been created with the help of the GungHo Consortium,
! whose members are identified at https://puma.nerc.ac.uk/trac/GungHo/wiki
!-------------------------------------------------------------------------------

!> @brief Provides a partitioning class

!> @details When instantiated, this module partitions the mesh for the
!> supported mesh types: bi-periodic plane and cubed sphere.
!> It provides a list of cells known to this partition. The order of the
!> list is as follows:
!> The cells that are wholly owned by the partition are followed by the cells
!> that might have dofs in the halo and then, the cells that form the
!> the halo. Finally, an extra halo depth of cells is generated, these are
!> called the ghost cells - they are not part of the partitioned domain, but are
!> required to fully describe the cells in the partitioned domain. The first
!> depth of halos are generated by applying a stencil to the owned cells.
!> Subsequent depths of halo are generated by applying the stencil to the
!> previous depth of halo cells

module partition_mod

use global_mesh_mod, only : global_mesh_type
use log_mod,         only : log_event,         &
                            LOG_LEVEL_ERROR
use constants_mod,   only: i_def, r_def

use ESMF

implicit none

private

public :: partitioner_cubedsphere, &
          partitioner_biperiodic, &
          partitioner_cubedsphere_serial, &
          partitioner_interface

type, public :: partition_type
  private
!> The number of the MPI rank
  integer              :: local_rank
!> Total number of MPI ranks in this execution
  integer              :: total_ranks
!> A List of global cell ids known to this partition ordered with core cells
!> first followed by the owned cells and finally the halo cells ordered by
!> depth of halo
  integer, allocatable :: global_cell_id( : )
!> A list of the ranks that own all the cells known to this partition
!> held in the order of cells in the <code>global_cell_id</code> array
  integer, pointer     :: cell_owner( : )
!> The number of "core" cells in the <code>global_cell_id</code> list
  integer              :: num_core
!> The number of "owned" cells in the <code>global_cell_id</code> list
  integer              :: num_owned
!> The number of "halo" cells in the <code>global_cell_id</code> list -
!> one entry for each depth of halo
  integer, allocatable :: num_halo( : )
!> The depth to which halos are generated
  integer              :: halo_depth
!> The number of "ghost" cells in the <code>global_cell_id</code> list
  integer(i_def)       :: num_ghost
!> The total number of cells in the global domain
  integer              :: global_num_cells

!-------------------------------------------------------------------------------
! Contained functions/subroutines
!-------------------------------------------------------------------------------
contains
  !> @brief  Returns the total of core, owned and all halo cells in a 2d slice on
  !> the local partition
  !> @return num_cells The total number of the core, owned and all halo cells
  !> on the local partition
  procedure, public :: get_num_cells_in_layer

  !> @brief  Returns the total number of core cells in a 2d slice on the local partition
  !> @return core_cells The total number of core cells on the local partition
  procedure, public :: get_num_cells_core

  !> @brief  Returns the total number of owned cells in a 2d slice on the local partition
  !> @return core_cells The total number of owned cells on the local partition
  procedure, public :: get_num_cells_owned

  !> @brief Returns the maximum depth of the halo
  !> @return halo_depth The maximum depth of halo cells
  procedure, public :: get_halo_depth

  !> @brief  Gets number of cells in a halo 
  !> @details Returns the total number of halo cells in a particular depth of halo in a
  !> 2d slice on the local partition
  !> @param[in] depth The depth of the halo being queried
  !> @return halo_cells The total number of halo cells of the particular depth
  !> on the local partition
  procedure, public :: get_num_cells_halo

  !> @brief Gets the total number of ghost cells in a slice around the local partition
  !> @return ghost_cells The total number of ghost cells around the local partition
  procedure, public :: get_num_cells_ghost

  !> @brief Returns the local rank number
  !> @return local_rank The number of the local rank
  procedure, public :: get_local_rank

  !> @brief Returns the total number of ranks
  !> @return total_ranks The total number of ranks
  procedure, public :: get_total_ranks

  !> @brief  Returns the owner of a cell on the local partition
  !> @param[in] cell_number The local id of of the cell being queried
  !> @return cell_owner The owner of the given cell
  procedure, public :: get_cell_owner

  !> @brief  Returns the global index of the cell that corresponds to the given
  !! local index on the local partition
  !> @param[in] lid The id of a cell in local index space
  !> @return gid The id of a cell in global index space
  procedure, public :: get_gid_from_lid

  !> @brief  Returns the local index of the cell on the local
  !> partition that corresponds to the given global index.
  !> @param[in] gid The global index to search for on the local partition
  !> @return lid The local index that corresponds to the given global index
  !> or -1 if the cell with the given global index is not present of the local partition
  procedure, public :: get_lid_from_gid

end type partition_type

interface partition_type
  module procedure partition_constructor
  module procedure partition_constructor_unit_test_data
end interface

interface
!-------------------------------------------------------------------------------
! Interface for the partitioner function pointer that is supplied to the constructor
!-------------------------------------------------------------------------------
  subroutine partitioner_interface( global_mesh, &
                                    xproc, &
                                    yproc, &
                                    local_rank, &
                                    total_ranks, &
                                    halo_depth, &
                                    global_cell_id, &
                                    num_core, &
                                    num_owned, &
                                    num_halo, &
                                    num_ghost )
    import :: global_mesh_type
    import :: i_def

    type(global_mesh_type), intent(in) :: global_mesh
    integer,                intent(in)    :: xproc, yproc, local_rank, total_ranks
    integer, allocatable,   intent(inout) :: global_cell_id( : )
    integer,                intent(in)    :: halo_depth
    integer(i_def),         intent(out)   :: num_core, num_owned, &
                                             num_halo( : ), num_ghost
  end subroutine partitioner_interface
end interface

contains

!-------------------------------------------------------------------------------
! Constructs partition object
!-------------------------------------------------------------------------------
!> @brief Constructs a <code>partition_type</code> object
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] partitioner A function pointer to the function that will perform
!>                         the partitioning
!> @param [in] xproc Number of ranks to partition the mesh over in the
!>                   x-direction (across a single face  for a cubed-sphere mesh)
!> @param [in] yproc Number of ranks to partition the mesh over in the
!>                   y-direction (across a single face  for a cubed-sphere mesh)
!> @param [in] halo_depth The depth to which halos will be created
!> @param [in] local_rank Number of the local process rank
!> @param [in] total_ranks Total number of process ranks available
!> @return self the partition object
!-------------------------------------------------------------------------------
function partition_constructor( global_mesh, &
                                partitioner, &
                                xproc, &
                                yproc, &
                                halo_depth, &
                                local_rank, &
                                total_ranks) result(self)

implicit none

type(global_mesh_type), intent(in) :: global_mesh

procedure(partitioner_interface) :: partitioner

integer, intent(in) :: xproc
integer, intent(in) :: yproc
integer, intent(in) :: halo_depth
integer, intent(in) :: local_rank
integer, intent(in) :: total_ranks

type(partition_type) :: self

type(ESMF_DistGrid) :: distgrid
integer :: rc
type(ESMF_Array) :: temporary_esmf_array
type(ESMF_RouteHandle) :: haloHandle
integer :: cell
integer(i_def) :: halo_start, halo_finish

self%local_rank = local_rank
self%total_ranks = total_ranks
self%halo_depth = halo_depth
allocate( self%num_halo(halo_depth) )
self%global_num_cells = global_mesh%get_ncells()

! Call the partitioner that has been passed into the routine
! as a procedure pointer
call partitioner( global_mesh, &
                  xproc, yproc, &
                  local_rank, &
                  total_ranks, &
                  halo_depth, &
                  self%global_cell_id, &
                  self%num_core, &
                  self%num_owned, &
                  self%num_halo, &
                  self%num_ghost )

! Calculate ownership of cells known to the local partition
! by filling the locally owned cells with the local rank and performing
! a halo-swap to fill in the owners of all the halo cells.
!
! Set up the ESMF structures required to perform a halo swap
!
rc = ESMF_SUCCESS
! Create an ESMF DistGrid, which describes which partition owns which cells
distgrid = ESMF_DistGridCreate( arbSeqIndexList= &
                          self%global_cell_id(1:self%num_core+self%num_owned), &
                                rc=rc )

halo_start  = self%num_core+self%num_owned+1
halo_finish = self%get_num_cells_in_layer()+self%get_num_cells_ghost()
!If this is a serial run (no halos), halo_start is out of bounds - so fix it
if(halo_start > self%get_num_cells_in_layer())then
  halo_start  = self%get_num_cells_in_layer()
  halo_finish = self%get_num_cells_in_layer() - 1
end if
! Can only halo-swap an ESMF array so set one up that's big enough to hold all
! the owned cells and all the halos
if (rc == ESMF_SUCCESS) &
  temporary_esmf_array = &
    ESMF_ArrayCreate( distgrid=distgrid, &
                      typekind=ESMF_TYPEKIND_I4, &
                      haloSeqIndexList= &
                                self%global_cell_id( halo_start:halo_finish ), &
                      rc=rc )

! Point our Fortran array at the space we've set up in the ESMF array
if (rc == ESMF_SUCCESS) &
  call ESMF_ArrayGet( array=temporary_esmf_array, &
                      farrayPtr=self%cell_owner, &
                      rc=rc )

! Calculate the routing table required to perform the halo-swap, so the
! code knows where to find the values it needs to fill in the halos
if (rc == ESMF_SUCCESS) then
  call ESMF_ArrayHaloStore( array=temporary_esmf_array, &
                            routehandle=haloHandle, &
                            rc=rc )

  ! Set ownership of all core + owned cells to the local rank id
  ! - halo cells are unset
  do cell = 1,self%num_core+self%num_owned
    self%cell_owner(cell)=local_rank
  end do
end if

! Do the halo swap to fill in the halo cell ownership
if (rc == ESMF_SUCCESS) &
  call ESMF_ArrayHalo( temporary_esmf_array, &
                       routehandle=haloHandle, &
                       rc=rc )

! Return code indicates something went wrong in the above, so log an error
if (rc /= ESMF_SUCCESS) call log_event( &
  'Failed to ascertain the ownership of halos in the partitioner.', &
  LOG_LEVEL_ERROR )

end function partition_constructor

!-------------------------------------------------------------------------------
! Biperiodic partitioner
!-------------------------------------------------------------------------------
!> Partitions the mesh on a bi-periodic plane. It returns the global ids of
!> the cells in the given partition.
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] xproc Number of processors along x-direction
!> @param [in] yproc Number of processors along y-direction
!> @param local_rank [in] Local MPI rank number
!> @param total_ranks [in] Total number of MPI ranks
!> @param [in] halo_depth The depth to which halos will be created
!> @param partitioned_cells [out] Returned array that holds the global ids of all
!>                          cells in local partition
!> @param num_core [out] Number of cells that are wholly owned by the partition
!>                 (i.e. all dofs in these cells are wholly owned by the
!>                 partition). The cell ids of these cells appear first in the
!>                 <code>partitioned_cells</code> array
!> @param num_owned [out] Number of cells that are owned by the partition,
!>                  but may have dofs that are also owned by halo cells. The
!>                  cell ids of these cells follow the core cells in the
!>                  <code>partitioned_cells</code> array
!> @param num_halo [out] Number of cells that are halo cells. The cell ids
!>                 of these cells follow the owned cells in the
!>                 <code>partitioned_cells</code> array
!> @param num_ghost [out] Number of "ghost" cells. These are cells in an extra
!>                  halo around the outermost actaul halo and are not in the
!>                  partitioned domain, but are required to fully describe the 
!>                  cells in the partitioned domain
!-------------------------------------------------------------------------------
  subroutine partitioner_biperiodic( global_mesh, &
                                     xproc, &
                                     yproc, &
                                     local_rank, &
                                     total_ranks, &
                                     halo_depth, &
                                     partitioned_cells, &
                                     num_core, &
                                     num_owned, &
                                     num_halo, &
                                     num_ghost )
  implicit none

  type(global_mesh_type), intent(in) :: global_mesh

  integer,               intent(in)    :: xproc
  integer,               intent(in)    :: yproc
  integer,               intent(in)    :: local_rank
  integer,               intent(in)    :: total_ranks
  integer,               intent(in)    :: halo_depth
  integer, allocatable,  intent(inout) :: partitioned_cells( : )
  integer,               intent(out)   :: num_core
  integer,               intent(out)   :: num_owned
  integer,               intent(out)   :: num_halo( : )
  integer(i_def),        intent(out)   :: num_ghost

  call partitioner_rectangular_panels( global_mesh, &
                                       1, &
                                       xproc, &
                                       yproc, &
                                       local_rank, &
                                       total_ranks, &
                                       halo_depth, &
                                       partitioned_cells, &
                                       num_core, &
                                       num_owned, &
                                       num_halo, &
                                       num_ghost )

  end subroutine partitioner_biperiodic

!-------------------------------------------------------------------------------
! Cubed sphere partitioner
!-------------------------------------------------------------------------------
!> Partitions the mesh on cubed_sphere. It returns the global ids of
!> the cells in the given partition.
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] xproc Number of processors along x-direction
!> @param [in] yproc Number of processors along y-direction
!> @param local_rank [in] Local MPI rank number
!> @param total_ranks [in] Total number of MPI ranks
!> @param [in] halo_depth The depth to which halos will be created
!> @param partitioned_cells [out] Returned array that holds the global ids of all
!>                          cells in local partition
!> @param num_core [out] Number of cells that are wholly owned by the partition
!>                 (i.e. all dofs in these cells are wholly owned by the
!>                 partition). The cell ids of these cells appear first in the
!>                 <code>partitioned_cells</code> array
!> @param num_owned [out] Number of cells that are owned by the partition,
!>                  but may have dofs that are also owned by halo cells. The
!>                  cell ids of these cells follow the core cells in the
!>                  <code>partitioned_cells</code> array
!> @param num_halo [out] Number of cells that are halo cells. The cell ids
!>                 of these cells follow the owned cells in the
!>                 <code>partitioned_cells</code> array
!> @param num_ghost [out] Number of "ghost" cells. These are cells in an extra
!>                  halo around the outermost actaul halo and are not in the
!>                  partitioned domain, but are required to fully describe the 
!>                  cells in the partitioned domain
!-------------------------------------------------------------------------------
  subroutine partitioner_cubedsphere( global_mesh, &
                                      xproc, &
                                      yproc, &
                                      local_rank, &
                                      total_ranks, &
                                      halo_depth, &
                                      partitioned_cells, &
                                      num_core, &
                                      num_owned, &
                                      num_halo, &
                                      num_ghost )
  implicit none

  type(global_mesh_type), intent(in) :: global_mesh

  integer,               intent(in)    :: xproc
  integer,               intent(in)    :: yproc
  integer,               intent(in)    :: local_rank
  integer,               intent(in)    :: total_ranks
  integer,               intent(in)    :: halo_depth
  integer, allocatable,  intent(inout) :: partitioned_cells( : )
  integer,               intent(out)   :: num_core
  integer,               intent(out)   :: num_owned
  integer,               intent(out)   :: num_halo( : )
  integer(i_def),        intent(out)   :: num_ghost

  !check that we have a number of ranks that is compatible with this partitioner
  if( modulo(total_ranks,6) /= 0 ) call log_event( &
  'The cubed-sphere partitioner requires a multiple of six processors.', &
  LOG_LEVEL_ERROR )

  call partitioner_rectangular_panels( global_mesh, &
                                       6, &
                                       xproc, &
                                       yproc, &
                                       local_rank, &
                                       total_ranks, &
                                       halo_depth, &
                                       partitioned_cells, &
                                       num_core, &
                                       num_owned, &
                                       num_halo, &
                                       num_ghost )

  end subroutine partitioner_cubedsphere

!-------------------------------------------------------------------------------
! Serial cubed sphere partitioner
!-------------------------------------------------------------------------------
!> Returns a single partition of cubed-sphere mesh for use when running the
!> code in serial
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] xproc Number of processors along x-direction
!> @param [in] yproc Number of processors along y-direction
!> @param local_rank [in] Local MPI rank number
!> @param total_ranks [in] Total number of MPI ranks
!> @param [in] halo_depth The depth to which halos will be created
!> @param partitioned_cells [out] Returned array that holds the global ids of all
!>                          cells in local partition
!> @param num_core [out] Number of cells that are wholly owned by the partition
!>                 (i.e. all dofs in these cells are wholly owned by the
!>                 partition). The cell ids of these cells appear first in the
!>                 <code>partitioned_cells</code> array
!> @param num_owned [out] Number of cells that are owned by the partition,
!>                  but may have dofs that are also owned by halo cells. The
!>                  cell ids of these cells follow the core cells in the
!>                  <code>partitioned_cells</code> array
!> @param num_halo [out] Number of cells that are halo cells. The cell ids
!>                 of these cells follow the owned cells in the
!>                 <code>partitioned_cells</code> array
!> @param num_ghost [out] Number of "ghost" cells. These are cells in an extra
!>                  halo around the outermost actaul halo and are not in the
!>                  partitioned domain, but are required to fully describe the 
!>                  cells in the partitioned domain
!-------------------------------------------------------------------------------
  subroutine partitioner_cubedsphere_serial( global_mesh, &
                                             xproc, &
                                             yproc, &
                                             local_rank, &
                                             total_ranks, &
                                             halo_depth, &
                                             partitioned_cells, &
                                             num_core, &
                                             num_owned, &
                                             num_halo, &
                                             num_ghost )

  use log_mod, only : log_event, LOG_LEVEL_ERROR

! The general partitioner for a cubed-sphere mesh returns a minimum of one
! partition per cubed-sphere "face". In order to run the code serially
! a special case for creating a single partition with all points is required.
! So here. we just return one big partition that holds everything
  implicit none

  type(global_mesh_type), intent(in) :: global_mesh

  integer,              intent(in)    :: xproc
  integer,              intent(in)    :: yproc
  integer,              intent(in)    :: local_rank
  integer,              intent(in)    :: total_ranks
  integer,              intent(in)    :: halo_depth
  integer, allocatable, intent(inout) :: partitioned_cells( : )
  integer,              intent(out)   :: num_core
  integer,              intent(out)   :: num_owned
  integer,              intent(out)   :: num_halo( : )
  integer(i_def),       intent(out)   :: num_ghost

  integer :: i
  integer :: depth ! loop counter over halo depths

  if( total_ranks /= 1 .or. local_rank /= 0 )then
   call log_event( 'Can only use the serial partitioner with a single process',&
     LOG_LEVEL_ERROR )
  endif

  if( xproc /= 1 .or. yproc /= 1)then
   call log_event( 'Invalid decomposition used for serial partitioner',&
     LOG_LEVEL_ERROR )
  endif

  num_core = global_mesh%get_ncells()
  num_owned = 0
  do depth = 1, halo_depth
    num_halo(depth) = 0
  end do
  num_ghost=0

  allocate(partitioned_cells(num_core))
  do i = 1,num_core
    partitioned_cells(i)= i
  end do

  end subroutine partitioner_cubedsphere_serial

!-------------------------------------------------------------------------------
! Helper routine that partitions a mesh that is formed from a number
! of rectangular panels.
!-------------------------------------------------------------------------------
  subroutine partitioner_rectangular_panels( global_mesh, &
                                             num_panels, &
                                             xproc, &
                                             yproc, &
                                             local_rank, &
                                             total_ranks, &
                                             halo_depth, &
                                             partitioned_cells, &
                                             num_core, &
                                             num_owned, &
                                             num_halo, &
                                             num_ghost )

  use linked_list_mod,       only : linked_list_type, add_item, add_unique_item, clear_list
  use reference_element_mod, only : W, E, N

  implicit none

  type(global_mesh_type), intent(in) :: global_mesh                ! A global mesh object 

  integer,                intent(in)    :: num_panels              ! Number of panels that make up the mesh
  integer,                intent(in)    :: xproc                   ! Number of processors along x-direction
  integer,                intent(in)    :: yproc                   ! Number of processors along y-direction
  integer,                intent(in)    :: local_rank              ! Local MPI rank number
  integer,                intent(in)    :: total_ranks             ! Total number of MPI ranks
  integer,                intent(in)    :: halo_depth              ! The depth to which halos will be created
  integer, allocatable,   intent(inout) :: partitioned_cells( : )  ! Returned array that holds the global ids of
                                                                   ! all cells in local partition
  integer,                intent(out)   :: num_core                ! Number of cells that are wholly owned by the partition
                                                                   ! (i.e. all dofs in these cells are wholly owned by the
                                                                   ! partition). The cell ids of these cells appear first in the
                                                                   ! partitioned_cells array
  integer,                intent(out)   :: num_owned               ! Number of cells that are owned by the partition,
                                                                   ! but may have dofs that are also owned by halo cells. The
                                                                   ! cell ids of these cells follow the core cells in the
                                                                   ! partitioned_cells array
  integer,                intent(out)   :: num_halo( : )           ! Number of cells that are halo cells. The cell ids
                                                                   ! of these cells follow the owned cells in the
                                                                   ! partitioned_cells array
  integer(i_def),         intent(out)   :: num_ghost               ! Number of cells that are ghost cells - surrounding,
                                                                   ! but not in the partitioned domain

  integer :: face      ! which face of the cube is implied by local_rank (0->5)
  integer :: start_cell ! lowest cell id of the face implaced by local_rank
  integer :: start_rank ! The number of the first rank on the face implied by local_rank
  integer :: local_xproc, local_yproc ! x- and y-dirn processor id of this partition
  integer :: start_x   ! global cell id of start of the domain on this partition in x-dirn
  integer :: num_x     ! number of cells in the domain on this partition in x-dirn
  integer :: start_y   ! global cell id of start of the domain on this partition in y-dirn
  integer :: num_y     ! number of cells in the domain on this partition in y-dirn
  integer :: num_in_list !total number of cells known to this partition
  integer :: num_added ! number of cells added to the linked list
  integer :: ix, iy    ! loop counters over cells on this partition in x- and y-dirns

  type(linked_list_type), pointer :: curr=>null()  ! the current position at which items will be added
                                                   ! to the list that holds cells known to this partition
  type(linked_list_type), pointer :: start=>null() ! start of the list that holds cells known to this partition
  type(linked_list_type), pointer :: last_core=>null() ! location of the last core cell in the list of cells
  type(linked_list_type), pointer :: last_owned=>null() ! location of the last owned cell in the list of cells
  type(linked_list_type), pointer :: last_halo=>null() ! location of the last halo cell in the list of cells
  type(linked_list_type), pointer :: curr_pos=>null() ! current position when looping over subsections of cells

  integer :: i, j         ! loop counters
  integer :: cells(4)     ! The cells around the vertex being queried
  integer :: oth1, oth2   ! When querying a cell around a vertex, these are
                          ! the indeces of the other two cells
  integer, allocatable :: sw_corner_cells(:)
                          ! List of cells at the SW corner of the panels
  integer :: panel        ! panel number
  integer :: cell         ! starting point for num_cells_x calculation
  integer :: cell_next(4) ! The cells around the cell being queried
  integer :: num_cells_x  ! number of cells across a panel in x-direction
  integer :: num_cells_y  ! number of cells across a panel in y-direction
  integer :: swap_temp    ! temporary swap space used to swap items in the bubble sort
  logical :: swapped      ! flag set to true if the current iteration of the bubble sort swapped any items
  integer :: start_sort, end_sort ! range over which to sort cells
  integer :: depth        ! counter over the halo depths
  integer :: orig_num_in_list ! number of cells in list before halos are added

  if(num_panels==1)then
    ! A single panelled mesh might be rectangluar - so find the dimensions
    ! Find num_cells_x by walking west-wards through the mesh until you're back where you started
    cell=1
    call global_mesh%get_cell_next(cell,cell_next)
    num_cells_x=1
    do i=1,global_mesh%get_ncells()
      if(cell_next(W)==cell)exit
      num_cells_x=num_cells_x+1
      call global_mesh%get_cell_next(cell_next(W),cell_next)
    end do
    ! Infer num_cells_y from the total domin size and num_cells_x
    num_cells_y=global_mesh%get_ncells()/num_cells_x
    ! make cell number 1 the South West corner cell of the single panel
    allocate(sw_corner_cells(1))
    sw_corner_cells(1)=1
  else
    ! For multi-panel meshes, the panels must be square
    num_cells_x = nint(sqrt(float(global_mesh%get_ncells())/float(num_panels)))
    num_cells_y = num_cells_x

    ! Calculate the South West corner cells of all the panels in the global mesh
    allocate(sw_corner_cells(num_panels))
    panel=1
    do i=1,global_mesh%get_nverts()
      call global_mesh%get_cell_on_vert(i,cells)
      if(cells(4) == 0)then
        do j=1,3
          call global_mesh%get_cell_next(cells(j),cell_next)
          oth1=j+1
          if(oth1>3)oth1=oth1-3
          oth2=j+2
          if(oth2>3)oth2=oth2-3
          if(cell_next(N)/=cells(oth1) .and. cell_next(N)/=cells(oth2) .and. &
             cell_next(E)/=cells(oth1) .and. cell_next(E)/=cells(oth2) ) then
            if(panel > num_panels) &
              call log_event( 'Failed to partition the mesh: '// &
                 'the global mesh has more panels than the partitioner '// &
                 'is expecting.', LOG_LEVEL_ERROR )
            sw_corner_cells(panel)=cells(j)
            panel=panel+1
          end if
        end do
      end if
    end do

  endif

  !convert the local rank number into a face number and a local xproc and yproc
  face = int(float(num_panels)*(real(local_rank)/real(total_ranks)))+1
  start_cell = sw_corner_cells(face)
  start_rank = xproc*yproc*(face-1)
  local_xproc = modulo(local_rank-start_rank,xproc)
  local_yproc = (local_rank-start_rank)/xproc

  deallocate(sw_corner_cells)

  !Work out the start index and number of cells (in x- and y-dirn) for
  !the local partition - this algorithm should spread out the number of
  !cells each partition gets fairly evenly
  start_x = int( ( real( local_xproc ) * real( num_cells_x )/ &
                   real( xproc ) ) + 0.5_r_def ) + 1
  num_x   = int( ( real( local_xproc + 1 ) * real( num_cells_x )/ &
                   real( xproc ) ) + 0.5_r_def ) - start_x + 1

  start_y = int( ( real( local_yproc ) * real( num_cells_y )/ &
                   real( yproc ) ) + 0.5_r_def ) + 1
  num_y   = int( ( real( local_yproc + 1 ) * real( num_cells_y )/ &
                   real( yproc ) ) + 0.5_r_def ) - start_y + 1

  !Create a linked list of cells known to this partition
  !Start with the core cells - those with all dofs wholly owned by the partition
  num_in_list = 0
  do iy = start_y + 1, start_y+num_y - 2
    do ix = start_x + 1, start_x+num_x - 2
      call add_item( curr,global_mesh%get_cell_id(start_cell, ix-1, iy-1) )
      if(.not.associated(start))start => curr
      num_in_list = num_in_list + 1
    end do
  end do
  num_core = num_in_list

  ! Store location of last core cell in the linked list
  last_core => curr

  ! Now add the owned cells - those still owned by the partition -
  ! but may have dofs shared with halo cells
  ! Those cells along the top/bottom
  do ix = start_x, start_x+num_x-1
    call add_item( curr,global_mesh%get_cell_id(start_cell, ix-1, start_y-1) )
    num_in_list = num_in_list+1
    if(.not.associated(start))start => curr
    call add_unique_item( start,curr,global_mesh%get_cell_id(start_cell, ix-1, start_y+num_y-2), num_added )
    num_in_list = num_in_list+num_added
  end do
  ! Those along the left/right
  do iy = start_y+1, start_y+num_y-2
    call add_unique_item( start,curr,global_mesh%get_cell_id(start_cell, start_x-1, iy-1), num_added )
    num_in_list = num_in_list+num_added
    call add_unique_item( start,curr,global_mesh%get_cell_id(start_cell, start_x+num_x-2, iy-1), num_added )
    num_in_list = num_in_list+num_added
  end do
  num_owned = num_in_list-num_core

  ! Store location of last owned cell in the linked list
  last_owned => curr

  ! Add all cells that are in a single depth halo around each of the owned cells
  ! Start by applying a stencil around the first cell after the last core cell
  ! (i.e. the first owned cell) - or the first cell (if there are no core cells)
  if(associated(last_core))then
   curr_pos => last_core%next
  else
   curr_pos => start
  end if
  orig_num_in_list = num_in_list
  call apply_stencil( global_mesh, &
                      curr_pos, &
                      num_owned, &
                      start, &
                      curr, &
                      num_in_list )

  num_halo(1) = num_in_list - orig_num_in_list

  curr_pos => last_owned%next
  do depth = 2,halo_depth+1
    ! Store location of last halo cell in the linked list
    last_halo => curr
    orig_num_in_list = num_in_list
    call apply_stencil( global_mesh, &
                        curr_pos, &
                        num_halo(depth-1), &
                        start, &
                        curr, &
                        num_in_list )
    if(depth <= halo_depth) then
      num_halo(depth) = num_in_list - orig_num_in_list
    else
      num_ghost = num_in_list - orig_num_in_list
    end if
    curr_pos => last_halo%next
  end do

  allocate(partitioned_cells(num_in_list))
  curr => start
  do i = 1,num_in_list
    partitioned_cells(i) = curr%dat
    curr => curr%next
  end do

  ! Deallocate the list
  call clear_list( start )

  ! Cell ids within the separate groups have to be in numerical order.
  ! so (bubble) sort the separate groups
  !
  !Sort core cells
  start_sort = 1
  end_sort = num_core
  do
    swapped = .false.
    do i = start_sort,end_sort-1
      if(partitioned_cells(i) > partitioned_cells(i+1))then
        swap_temp = partitioned_cells(i)
        partitioned_cells(i) = partitioned_cells(i+1)
        partitioned_cells(i+1) = swap_temp
        swapped = .true.
      end if
    end do
    if( .not.swapped )exit
  end do
  !
  ! Sort owned cells
  start_sort = num_core + 1
  end_sort = num_core + num_owned
  do
    swapped = .false.
    do i = start_sort,end_sort-1
      if(partitioned_cells(i) > partitioned_cells(i+1))then
        swap_temp = partitioned_cells(i)
        partitioned_cells(i) = partitioned_cells(i+1)
        partitioned_cells(i+1) = swap_temp
        swapped = .true.
      end if
    end do
    if( .not.swapped )exit
  end do
  !
  ! Sort halo cells in their groups of halo depths
  do depth = 1,halo_depth +1      ! also sort ghost cells
    start_sort = end_sort + 1
    if(depth <= halo_depth) then
      end_sort = start_sort + num_halo(depth) - 1
    else
      end_sort = start_sort + num_ghost - 1
    end if
    do
      swapped = .false.
      do i = start_sort,end_sort-1
        if(partitioned_cells(i) > partitioned_cells(i+1))then
          swap_temp = partitioned_cells(i)
          partitioned_cells(i) = partitioned_cells(i+1)
          partitioned_cells(i+1) = swap_temp
          swapped = .true.
        end if
      end do
      if( .not.swapped )exit
    end do
  end do

  end subroutine partitioner_rectangular_panels

!-------------------------------------------------------------------------------
! Applies a stencil around a collection of cells. PRIVATE subroutine.
!-------------------------------------------------------------------------------
! Details: Applies a single depth stencil around a collection of cells and adds
!          the global ids of the stencil cells to a list of cells known to the 
!          partition - if they are not already in the list.
! Input:   global_mesh      A global mesh object that describes the layout of 
!                           the global mesh
!          input_cells      A pointer to the start of a portion of the linked list
!                           over which the stencil will be applied
!          number_of_cells  The number of cells in the portion of the linked list
!                           over which the stencil will be applied
!          start            Start of the linked-list that holds cells known to 
!                           this partition
! In/Out:  curr             Current position at which items will be added to the
!                           list that holds cells known to this partition
!          num_in_list      Total number of cells known to this partition
!-------------------------------------------------------------------------------
  subroutine apply_stencil( global_mesh, &
                            input_cells, &
                            number_of_cells, &
                            start, &
                            curr, &
                            num_in_list )
  use linked_list_mod, only : linked_list_type, add_unique_item
  use reference_element_mod, only : nverts_h
  implicit none

  type(global_mesh_type), intent(in)             :: global_mesh
  type(linked_list_type), intent(inout), pointer :: input_cells
  integer,                intent(in)             :: number_of_cells
  type(linked_list_type), intent(inout), pointer :: curr
  type(linked_list_type), intent(in),    pointer :: start
  integer,                intent(inout)          :: num_in_list

  integer              :: i,j,k  ! loop counter
  integer              :: cell_id ! the current cell id that the stencil is being applied to
  integer              :: num_added ! number of cells added to the linked list
  integer, allocatable :: verts(:)
  integer, allocatable :: cells(:)

  allocate( cells( global_mesh%get_max_cells_per_vertex() ) )
  allocate( verts(nverts_h) )

  do i = 1,number_of_cells
    cell_id = input_cells%dat
    call global_mesh%get_vert_on_cell(cell_id, verts)
    do j = 1,nverts_h
      call global_mesh%get_cell_on_vert( verts(j), cells )
      do k = 1,global_mesh%get_max_cells_per_vertex()
        if(cells(k) > 0)then
          call add_unique_item( start,curr,cells(k), num_added )
          num_in_list = num_in_list + num_added
        end if
      end do
    end do
    input_cells => input_cells%next
  end do

  deallocate(verts)
  deallocate(cells)

  end subroutine apply_stencil

!-------------------------------------------------------------------------------
! Gets total number of cells in a layer
!-------------------------------------------------------------------------------
function get_num_cells_in_layer( self ) result ( num_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: num_cells
  integer :: depth   ! loop counter over halo depths

  num_cells = self%num_core + self%num_owned

  do depth = 1,self%halo_depth
    num_cells = num_cells + self%num_halo(depth)
  end do

end function get_num_cells_in_layer

!-------------------------------------------------------------------------------
! Gets total number of core cells in a 2d slice on the local partition
!-------------------------------------------------------------------------------
function get_num_cells_core( self ) result ( core_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: core_cells

  core_cells = self%num_core

end function get_num_cells_core

!-------------------------------------------------------------------------------
! Gets total number of owned cells in a 2d slice on the local partition
!-------------------------------------------------------------------------------
function get_num_cells_owned( self ) result ( owned_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: owned_cells

  owned_cells = self%num_owned

end function get_num_cells_owned


!-------------------------------------------------------------------------------
! Gets the depth of the halos that have been set up
!-------------------------------------------------------------------------------
function get_halo_depth( self ) result ( halo_depth )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: halo_depth

  halo_depth = self%halo_depth

end function get_halo_depth


!-------------------------------------------------------------------------------
! Gets total number of halo cells in a particular depth of halo in a 2d
! slice on the local partition
!-------------------------------------------------------------------------------
function get_num_cells_halo( self, depth ) result ( halo_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: depth
  integer             :: halo_cells

  if( depth > self%halo_depth )then
    halo_cells = 0
  else
    halo_cells = self%num_halo(depth)
  end if

end function get_num_cells_halo


!-------------------------------------------------------------------------------
! Gets total number of ghost cells in a 2d slice on the local partition
!-------------------------------------------------------------------------------
function get_num_cells_ghost( self ) result ( ghost_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer(i_def) :: ghost_cells

  ghost_cells = self%num_ghost

end function get_num_cells_ghost


!-------------------------------------------------------------------------------
! Gets the local rank number
!-------------------------------------------------------------------------------
function get_local_rank( self ) result ( local_rank )
  implicit none

  class(partition_type), intent(in) :: self

  integer             :: local_rank

  local_rank = self%local_rank

end function get_local_rank


!-------------------------------------------------------------------------------
! Gets the total number of ranks being used
!-------------------------------------------------------------------------------
function get_total_ranks( self ) result ( total_ranks )
  implicit none

  class(partition_type), intent(in) :: self

  integer             :: total_ranks

  total_ranks = self%total_ranks

end function get_total_ranks

!-------------------------------------------------------------------------------
! Gets the owner of a cell on the local partition
!-------------------------------------------------------------------------------
function get_cell_owner( self, cell_number ) result ( cell_owner )

  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: cell_number

  integer :: cell_owner

  cell_owner=self%cell_owner(cell_number)

end function get_cell_owner

!-------------------------------------------------------------------------------
! Gets the global index of the cell that corresponds to the given
! local index on the local partition
!-------------------------------------------------------------------------------
function get_gid_from_lid( self, lid ) result ( gid )

  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: lid           ! local index
  integer             :: gid           ! global index
  integer             :: nlayer        ! layer of supplied lid
  integer             :: lid_in_layer  ! supplied lid projected to bottom layer
  integer             :: num_in_list   ! total number of cells in partition
  integer             :: depth         ! loop counter over halo depths

  num_in_list = self%num_core + self%num_owned + self%num_ghost
  do depth = 1,self%halo_depth
    num_in_list = num_in_list + self%num_halo(depth)
  end do
  lid_in_layer = modulo(lid-1,(num_in_list))+1
  nlayer = (lid-1)/(num_in_list)

  gid = self%global_cell_id(lid_in_layer) + nlayer*(self%global_num_cells)

end function get_gid_from_lid

!-------------------------------------------------------------------------------
! Gets the local index of the cell on the local partition that corresponds
! to the given global index
!-------------------------------------------------------------------------------
function get_lid_from_gid( self, gid ) result ( lid )
!
! Performs a search through the global cell lookup table looking for the
! required global index.
!
! The partitioned_cells array holds global indices in four groups: core cells,
! followed by owned cells, then the halo cells and fnally the ghost cells. The
! cells are numerically ordered within the different groups so a binary search
! can be used, but not between groups, so need to do separate binary searches
! through the core, owned, halo and ghost cells and exit if a match is found
!
  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: gid           ! global index
  integer             :: lid           ! local index
  integer             :: nlayer        ! layer of supplied gid
  integer             :: gid_in_layer  ! supplied gid projected to bottom layer
  integer             :: depth         ! loop counter over halo depths
  integer             :: start_search  ! start point for a search
  integer             :: end_search    ! end point for a search
  integer             :: num_in_list   ! total number of cells in partition
  integer(i_def)      :: num_halo      ! number of halo points already counted

  num_in_list = self%num_core + self%num_owned
  do depth = 1,self%halo_depth
    num_in_list = num_in_list + self%num_halo(depth)
  end do

  ! Set the default return code
  lid = -1
  ! If the supplied gid is not valid just return
  if(gid < 1) return

  ! The global index lookup table (partitioned_cells) only has the indices for
  ! a single layer, so convert the full 3d global index into the global index
  ! within the layer and a layer number
  gid_in_layer = modulo(gid-1,self%global_num_cells) + 1
  nlayer = (gid-1) / self%global_num_cells

  ! Search though core cells - looking for the gid
  start_search = 1
  end_search = self%num_core
  lid = binary_search( self%global_cell_id( start_search:end_search ), gid )
  if(lid /= -1)then
    lid = lid + nlayer*(num_in_list)  !convert back to 3d lid
    return
  end if

  ! Search though owned cells - looking for the gid
  start_search = end_search + 1
  end_search = start_search + self%num_owned - 1
  lid = binary_search( self%global_cell_id( start_search:end_search ), gid )
  if(lid /= -1)then
    lid = lid + self%num_core + nlayer*(num_in_list)  !convert back to 3d lid
    return
  end if

  ! Search though halo and ghost cells - looking for the gid
  num_halo=0
  do depth = 1,self%halo_depth +1
    start_search = end_search + 1
    if(depth <= self%halo_depth) then
      end_search = start_search + self%num_halo(depth) - 1
    else
      end_search = start_search + self%num_ghost - 1
    end if
    lid = binary_search( self%global_cell_id( start_search:end_search ), gid )
    if(lid /= -1)then
      lid = lid + self%num_core + self%num_owned + num_halo + &
                                   nlayer*(num_in_list)  !convert back to 3d lid
      return
    end if
    if(depth <= self%halo_depth) then
      num_halo = num_halo + self%num_halo(depth)
    end if
  end do

  ! No lid has been found in either the core, owned or halo cells on this partition, so return with lid=-1
  return

end function get_lid_from_gid

!-------------------------------------------------------------------------------
! Performs a binary search through the given array. PRIVATE function.
!-------------------------------------------------------------------------------
! Details: Performs a binary search through the given array looking for a
!          particular entry and returns the index of the entry found or -1 if no
!          matching entry can be found. The values held in "array_to_be_searched"
!          must be in numerically increasing order.
! Input:   array_to_be_searched  The array that will be searched for the given entry
!          value_to_find         The entry that is to be searched for
!-------------------------------------------------------------------------------
pure function binary_search( array_to_be_searched, value_to_find ) result ( id )

  implicit none

  integer, intent(in) :: array_to_be_searched( : )
  integer, intent(in) :: value_to_find
  integer             :: bot, top  ! Lower and upper index limits between which to search for the value
  integer             :: id        ! Next index for refining the search. If an entry is found this will
                                   ! contain the index of the matching entry

  ! Set bot and top to be the whole array to begin with
  bot = 1
  top = size(array_to_be_searched)

  search: do
    ! If top is lower than bot then there is no more array to be searched
    if(top < bot) exit search
    ! Refine the search
    id = (bot+top)/2
    if(array_to_be_searched(id) == value_to_find)then  ! found matching entry
      return
    else if(array_to_be_searched(id) < value_to_find)then ! entry has to be between id and top
      bot = id + 1
    else ! entry has to be between bot and id
      top = id - 1
    endif
  end do search

  ! Didn't find a match - return failure code
  id = -1

end function binary_search


!==============================================================================
! The following routines are only available when setting data for unit testing.
!==============================================================================
!> @brief   Stucture-Constructor (for unit testing)
!> @returns A partition object based on a 9-cell global mesh (3x3) with one
!>          partition and quadralateral reference cells
!============================================================================
function partition_constructor_unit_test_data() result (self)

  implicit none

  type(partition_type) :: self

  ! Returns partition object from global_mesh of size 3x3 quad reference cell
  ! (see global_mesh_mod for data) which only has one partition.

  self%local_rank       = 0
  self%total_ranks      = 1
  self%num_core         = 9
  self%num_owned        = 0
  self%halo_depth       = 1
  self%global_num_cells = 9

  allocate( self%global_cell_id (self%global_num_cells) )
  allocate( self%cell_owner     (self%global_num_cells) )
  allocate( self%num_halo       (self%halo_depth) )

  self%global_cell_id   = [1,2,3,4,5,6,7,8,9]
  self%cell_owner       = [0,0,0,0,0,0,0,0,0]
  self%num_halo(:)      = 0
  self%num_ghost        = 0

end function partition_constructor_unit_test_data

end module partition_mod
